DESCRIBE keyspaces;
CREATE  KEYSPACE test_schema with replication={'class' : 'SimpleStrategy', 'replication_factor' : 1 };

CREATE TABLE test_table(
   column1 int,
   column2 varchar,
   column3 varchar,
   PRIMARY KEY (column1)
   );

describe tables;

insert into test_schema.test_table (column1 ,column2 ,column3) values (1,'anand','cse') ;
select * from test_schema.test_table;


insert into test_schema.test_table (column1 ,column2 ,column3) values (2,'irfan','cse') ;
insert into test_schema.test_table (column1 ,column2 ,column3) values (1,'irfan','cse') ; [ overwrite the existing record]

update test_schema.test_table set column2='anand_new' where column1=3; [ insert the record thought col1=3 does not exist]

update test_schema.test_table set column1='4' where column1=3;

delete from test_schema.test_table where column1=1; [partition key is mandatory in filter condition]

drop table test_schema.test_table;

=========================================================

spark + cassandra:
========================

Create a keyspace called “test_schema” in Cassandra:
CREATE TABLE test_table1(
   column1 int,
   column2 varchar,
   column3 varchar,
   PRIMARY KEY (column1)
   );

insert into test_schema.test_table (column1 ,column2 ,column3) values (1,'anand','cse') ;

spark-shell --jars "/root/cassandra/new_tar/spark-cassandra-connector_2.10-1.6.3.jar,/root/cassandra/new_tar/jsr166e-1.1.0.jar"          [ compatible version spark 1.6, cassandra 3.11.0]


sc.stop
import com.datastax.spark.connector._, org.apache.spark.SparkContext, org.apache.spark.SparkContext._, org.apache.spark.SparkConf
val conf = new SparkConf(true).set("spark.cassandra.connection.host", "localhost")
val sc = new SparkContext(conf);
val test_spark_rdd = sc.cassandraTable("test_schema","test_table");
test_spark_rdd.first



To read data from Cassandra, you create an RDD from a specific table.  This is very simple:
val data = sc.cassandraTable(“my_keyspace”, “my_table”)
This will return an RDD of type Row, where Row is a data type which stores a single row from a table as a map from column name to value.

to read cassandra table to dataframe
import org.apache.spark.sql.SQLContext;
val sqlContext1=new SQLContext(sc)
val df = sqlContext1.read.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "test_table", "keyspace" -> "test_schema")).load()

Writing data to Cassandra is just as simple: on an RDD call the function saveToCassandra and specify the keycap and the table.  
Make sure that the type of the RDD maps to the Cassandra table.
test_spark_rdd.saveToCassandra("test_schema","test_table1")
Will save the RDD of type Movie to the movies table in the keyspace spark_demo.


listen_address: 192.168.101.11 (192.168.101.12 on other cassandra node)
start_native_transport: true
native_transport_port: 9042
start_rpc: true
rpc_address: 192.168.101.11 (192.168.101.12 on other cassandra node)
rpc_port: 9160


DEFAULT_HOST = '127.0.0.1'

